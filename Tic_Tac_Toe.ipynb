{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2KCAyI4NGsdM",
        "outputId": "bf1f6212-921e-4516-bd71-ad6989de0f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the game...\n",
            "Agent -> x\n",
            "Human -> o\n",
            "Agent is playing with himself to get trained\n",
            "Agent has played 1000 times\n",
            "Agent has played 2000 times\n",
            "Agent has played 3000 times\n",
            "Agent has played 4000 times\n",
            "Agent has played 5000 times\n",
            "Agent has played 6000 times\n",
            "Agent has played 7000 times\n",
            "Agent has played 8000 times\n",
            "Agent has played 9000 times\n",
            "\n",
            "Agent is now trained by playing with himself 10,000 times\n",
            " ---------------------\n",
            "  |  x  |     |     |\n",
            " ---------------------\n",
            "  |     |     |     |\n",
            " ---------------------\n",
            "  |     |     |     |\n",
            " ---------------------\n",
            "\n",
            "\n",
            "Enter box location : 1,1\n",
            " ---------------------\n",
            "  |  x  |     |     |\n",
            " ---------------------\n",
            "  |     |  o  |  x  |\n",
            " ---------------------\n",
            "  |     |     |     |\n",
            " ---------------------\n",
            "\n",
            "\n",
            "Enter box location : 2,2\n",
            " ---------------------\n",
            "  |  x  |  x  |     |\n",
            " ---------------------\n",
            "  |     |  o  |  x  |\n",
            " ---------------------\n",
            "  |     |     |  o  |\n",
            " ---------------------\n",
            "\n",
            "\n",
            "Enter box location : 0,2\n",
            " ---------------------\n",
            "  |  x  |  x  |  o  |\n",
            " ---------------------\n",
            "  |     |  o  |  x  |\n",
            " ---------------------\n",
            "  |  x  |     |  o  |\n",
            " ---------------------\n",
            "\n",
            "\n",
            "Enter box location : 1,0\n",
            " ---------------------\n",
            "  |  x  |  x  |  o  |\n",
            " ---------------------\n",
            "  |  o  |  o  |  x  |\n",
            " ---------------------\n",
            "  |  x  |  x  |  o  |\n",
            " ---------------------\n",
            "\n",
            "\n",
            "Game number: 1\n",
            "Game is draw\n",
            "Restart game? [y/n]: n\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.epsilon = 0.1  # For choosing a random action or take a greedy action\n",
        "        self.alpha = 0.5  # Learning rate which will be used in our value function\n",
        "        self.state_history = []  # For keeping records of the steps\n",
        "    def initialize_V(self, env, state_winner_triples):\n",
        "        # initialize V\n",
        "        # if agent wins, V(s) = 1\n",
        "        # if agent loses or draw V(s) = 0\n",
        "        # otherwise V(s) = 0.5\n",
        "        V = np.zeros(env.max_states)\n",
        "        for state, winner, ended in state_winner_triples:\n",
        "            if ended:\n",
        "                if winner == env.x:  # x is our agent\n",
        "                    state_value = 1\n",
        "                else:\n",
        "                    state_value = 0\n",
        "            else:\n",
        "                state_value = 0.5\n",
        "            V[state] = state_value\n",
        "        self.V = V\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "    def reset_history(self):\n",
        "        self.state_history = []\n",
        "    def choose_random_action(self, env):\n",
        "        empty_moves = env.get_empty_moves()\n",
        "        random_index_from_empty_moves = np.random.choice(len(empty_moves))\n",
        "        next_random_move = empty_moves[random_index_from_empty_moves]\n",
        "        return next_random_move\n",
        "    def choose_best_action_from_states(self, env):\n",
        "        next_best_move, best_state = env.get_next_best_move(self)\n",
        "        return next_best_move, best_state\n",
        "    def get_next_move(self, env):\n",
        "        next_best_move, best_state = None, None\n",
        "        random_number = np.random.rand()\n",
        "        if random_number < self.epsilon:\n",
        "            next_best_move = self.choose_random_action(env)\n",
        "        else:\n",
        "            next_best_move, best_state = self.choose_best_action_from_states(env)\n",
        "        return next_best_move, best_state\n",
        "    def take_action(self, env):\n",
        "        selected_next_move, best_state = self.get_next_move(env)\n",
        "        env.board[selected_next_move[0], selected_next_move[1]] = self.symbol\n",
        "    def update_state_history(self, state):\n",
        "        self.state_history.append(state)\n",
        "    def update(self, env):\n",
        "        # we are only updating at the end of an episode\n",
        "        # we will backtrack over all the states to collect function value\n",
        "    # V(prev_state) = V(prev_state) + alpha * ( V(next_state) - V(pre_state) ), where V(next_state) is reward if its most current state\n",
        "        reward = env.reward(self.symbol)\n",
        "        target = reward\n",
        "        for prev in reversed(self.state_history):\n",
        "            value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
        "            self.V[prev] = value\n",
        "            target = value\n",
        "        self.reset_history()\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3))  # making an 2D array\n",
        "        self.x = -1  # Player 1\n",
        "        self.o = 1  # Player 2\n",
        "        self.winner = None  # Initially there will be no winner\n",
        "        self.ended = False  # Game is not ended initially\n",
        "        self.max_states = 3 ** (3 * 3)  # Total no, of possible states this game\n",
        "\n",
        "    def is_empty(self, i, j):\n",
        "        return self.board[i, j] == 0\n",
        "    def reward(self, symbol):\n",
        "        collected_reward = 0\n",
        "        if self.game_over() and self.winner == symbol:\n",
        "            collected_reward = 1\n",
        "        return collected_reward\n",
        "    def is_draw(self):\n",
        "        is_draw = False\n",
        "        if self.ended and self.winner is None:\n",
        "            is_draw = True\n",
        "        return is_draw\n",
        "    def get_state(self):\n",
        "        state = 0\n",
        "        loop_index = 0\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if self.board[i, j] == self.x:\n",
        "                    state_value = 1\n",
        "                elif self.board[i, j] == self.o:\n",
        "                    state_value = 2\n",
        "                else:\n",
        "                    state_value = 0\n",
        "                state += (3 ** loop_index) * state_value\n",
        "                loop_index += 1\n",
        "        return state\n",
        "    def game_over(self):\n",
        "        if self.ended:  # return True if this environment has ended ie if this game has ended\n",
        "            return True  # game is over\n",
        "        players = [self.x, self.o]\n",
        "        # checking Rows\n",
        "        for i in range(3):\n",
        "            for player in players:\n",
        "                if self.board[i].sum() == player * 3:\n",
        "                    self.winner = player\n",
        "                    self.ended = True\n",
        "                    return True  # Game is over\n",
        "        # checking Columns\n",
        "        for j in range(3):\n",
        "            for player in players:\n",
        "                if self.board[:, j].sum() == player * 3:\n",
        "                    self.winner = player\n",
        "                    self.ended = True\n",
        "                    return True  # Game is over\n",
        "        # Checking at diagonals\n",
        "        for player in players:\n",
        "            if self.board.trace() == player * 3:\n",
        "                self.winner = player\n",
        "                self.ended = True\n",
        "                return True  # Game is over\n",
        "            if np.fliplr(self.board).trace() == player * 3:\n",
        "                self.winner = player\n",
        "                self.ended = True\n",
        "                return True  # Game is over\n",
        "        board_with_true_false = self.board == 0\n",
        "        if np.all(board_with_true_false == False):\n",
        "            self.winner = None # game is draw hence there is no winner\n",
        "            self.ended = True\n",
        "            return True  # Game is over\n",
        "        self.winner = None\n",
        "        return False\n",
        "    def get_empty_moves(self):\n",
        "        empty_moves = []\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if self.is_empty(i, j):\n",
        "                    empty_moves.append((i, j))\n",
        "        return empty_moves\n",
        "\n",
        "    def get_next_best_move(self, agent):\n",
        "        best_value = -1\n",
        "        next_best_move = None\n",
        "        best_state = None\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if self.is_empty(i, j):\n",
        "                    self.board[i, j] = agent.symbol\n",
        "                    state = self.get_state()\n",
        "                    self.board[i, j] = 0\n",
        "                    if agent.V[state] > best_value:\n",
        "                        best_value = agent.V[state]\n",
        "                        best_state = state\n",
        "                        next_best_move = (i, j)\n",
        "        return next_best_move, best_state\n",
        "    def draw_board(self):\n",
        "        def __print(to_print, j):\n",
        "            if j == 0:\n",
        "                print(f\"|  {to_print}  \", end=\"|\")\n",
        "            else:\n",
        "                print(f\"{to_print}  \", end=\"|\")\n",
        "        for i in range(3):\n",
        "            print(\" ---------------------\")\n",
        "            for j in range(3):\n",
        "                print(\"  \", end=\"\")\n",
        "                if self.board[i, j] == self.x:\n",
        "                    __print('x', j)\n",
        "                elif self.board[i, j] == self.o:\n",
        "                    __print('o', j)\n",
        "                else:\n",
        "                    __print(' ', j)\n",
        "            print(\"\")\n",
        "        print(\" ---------------------\")\n",
        "        print(\"\\n\")\n",
        "class Human:\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "    def take_action(self, env):\n",
        "        while True:\n",
        "            try:\n",
        "                move = input(\"Enter box location : \")\n",
        "                i, j = [int(item.strip()) for item in move.split(',')]\n",
        "                if env.is_empty(i, j):\n",
        "                    env.board[i, j] = self.symbol\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Please enter move\")\n",
        "            except:\n",
        "                print(\"Please enter move\")\n",
        "def get_state_hash_and_winner(env, i=0, j=0):\n",
        "    results = []\n",
        "    for v in [0, env.x, env.o]:\n",
        "        env.board[i, j] = v\n",
        "        if j == 2:\n",
        "            if i == 2:\n",
        "                state = env.get_state()\n",
        "                ended = env.game_over()\n",
        "                winner = env.winner\n",
        "                results.append((state, winner, ended))\n",
        "            else:\n",
        "                results += get_state_hash_and_winner(env, i + 1, 0)\n",
        "        else:\n",
        "            results += get_state_hash_and_winner(env, i, j + 1)\n",
        "    return results\n",
        "def play_game(agent, human, env, print_board=True):\n",
        "    current_player = None\n",
        "    continue_game = True\n",
        "    while continue_game:\n",
        "        if current_player == agent:\n",
        "            current_player = human\n",
        "        else:\n",
        "            current_player = agent\n",
        "        current_player.take_action(env)\n",
        "        if current_player == agent:\n",
        "            state = env.get_state()\n",
        "            agent.update_state_history(state)  # Player 1 will be agent\n",
        "            agent.update(env)\n",
        "            if print_board:\n",
        "                env.draw_board()\n",
        "        if env.game_over():\n",
        "            continue_game = False\n",
        "def main(should_learn_before_playing):\n",
        "    print(\"Starting the game...\")\n",
        "    print(\"Agent -> x\")\n",
        "    print(\"Human -> o\")\n",
        "    env = Environment()\n",
        "    state_winner_triples = get_state_hash_and_winner(env)\n",
        "    agent = Agent()\n",
        "    agent.set_symbol(env.x)\n",
        "    agent.initialize_V(env, state_winner_triples)\n",
        "    if should_learn_before_playing:\n",
        "        print(\"Agent is playing with himself to get trained\")\n",
        "        agent_to_learn = Agent()\n",
        "        agent_to_learn.set_symbol(env.o)\n",
        "        agent_to_learn.initialize_V(env, state_winner_triples)\n",
        "        for i in range(10000):\n",
        "            if i > 0 and i % 1000 == 0:\n",
        "                print(f\"Agent has played {i} times\")\n",
        "            play_game(agent, agent_to_learn, Environment(), print_board=False)\n",
        "        print(\"\")\n",
        "        print(\"Agent is now trained by playing with himself 10,000 times\")\n",
        "    human = Human()\n",
        "    human.set_symbol(env.o)\n",
        "    total_game_played = 0\n",
        "    while True:\n",
        "        env = Environment()\n",
        "        play_game(agent, human, env=env)\n",
        "        total_game_played += 1\n",
        "        print(f\"Game number: {total_game_played}\")\n",
        "        if env.winner == env.x:\n",
        "            print(f\"Agent won the game\")\n",
        "        elif env.winner == env.o:\n",
        "            print(f\"You won the game\")\n",
        "        else:\n",
        "            print(f\"Game is draw\")\n",
        "        answer = input(\"Restart game? [y/n]: \")\n",
        "        if answer and answer.lower()[0] == 'n':\n",
        "            break\n",
        "if __name__ == '__main__':\n",
        "    main(should_learn_before_playing=True)\n"
      ]
    }
  ]
}